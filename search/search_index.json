{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ontology Based Application Starter Kit (OBASK) Guide Ontology Based Application Starter Kit (OBASK) is an innovative tool that empowers users to easily create search and knowledge exploration applications using their ontologies, without the need for any programming skills. With OBASK, users can quickly kickstart an ontology-driven application by simply defining their input ontologies and providing gross classifications via configuration files. Once the configuration is set up, OBASK takes care of the heavy lifting, automatically generating knowledge search endpoints, Solr indexes, and materialized knowledge graphs with a single command. This means that users can focus on exploring and analyzing their data without getting bogged down in technical details. With OBASK, users can harness the full potential of their ontologies to create powerful and insightful applications that enable them to better understand and analyze their data. OBASK Features Quick Start My Project Advanced Configurations Architecture","title":"Getting started"},{"location":"#ontology-based-application-starter-kit-obask-guide","text":"Ontology Based Application Starter Kit (OBASK) is an innovative tool that empowers users to easily create search and knowledge exploration applications using their ontologies, without the need for any programming skills. With OBASK, users can quickly kickstart an ontology-driven application by simply defining their input ontologies and providing gross classifications via configuration files. Once the configuration is set up, OBASK takes care of the heavy lifting, automatically generating knowledge search endpoints, Solr indexes, and materialized knowledge graphs with a single command. This means that users can focus on exploring and analyzing their data without getting bogged down in technical details. With OBASK, users can harness the full potential of their ontologies to create powerful and insightful applications that enable them to better understand and analyze their data. OBASK Features Quick Start My Project Advanced Configurations Architecture","title":"Ontology Based Application Starter Kit (OBASK) Guide"},{"location":"advanced_config/","text":"OBASK Advanced Settings Configuring Ontology Files in collectdata The OBASK pipeline allows you to configure both remote and local ontology files that will be used in your project. These files are configured as part of the collectdata module within the config/collectdata folder, specifically in the vfb_fullontologies.txt file, and are processed as follows: Remote Ontology Files To configure remote ontology files, add the URLs of the ontology files you want to download directly to the config/collectdata/vfb_fullontologies.txt file. Each URL should be on a new line. http://purl.obolibrary.org/obo/go.owl http://purl.obolibrary.org/obo/cl.owl When the pipeline runs, it will download these ontology files from the specified remote sources and process them accordingly. Local Ontology Files For local ontology files that you want to include in the pipeline, you should follow these steps: Place the Files : Copy the local ontology files into the config/collectdata/local_ontologies directory. The files should have a .owl extension. No Need to List in vfb_fullontologies.txt : Unlike remote files, you do not need to add the local files to the vfb_fullontologies.txt file. The pipeline automatically detects any .owl files in the local_ontologies directory. Pipeline Processing : During the pipeline execution, any .owl files found in the local_ontologies directory will be copied to the VFB_DOWNLOAD_DIR . After this, the pipeline will proceed to download and process any remote ontology files listed in vfb_fullontologies.txt . Example Configuration If your project requires both remote and local ontology files, your setup might look like this: Remote Files in vfb_fullontologies.txt : Local Files in config/collectdata/local_ontologies : Place files like my_local_ontology.owl and custom_terms.owl directly in the local_ontologies directory. This configuration ensures that your project uses a combination of remote and local ontology files, processed in a seamless and automated way during the pipeline run.","title":"Advanced settings"},{"location":"advanced_config/#obask-advanced-settings","text":"","title":"OBASK Advanced Settings"},{"location":"advanced_config/#configuring-ontology-files-in-collectdata","text":"The OBASK pipeline allows you to configure both remote and local ontology files that will be used in your project. These files are configured as part of the collectdata module within the config/collectdata folder, specifically in the vfb_fullontologies.txt file, and are processed as follows:","title":"Configuring Ontology Files in collectdata"},{"location":"advanced_config/#remote-ontology-files","text":"To configure remote ontology files, add the URLs of the ontology files you want to download directly to the config/collectdata/vfb_fullontologies.txt file. Each URL should be on a new line. http://purl.obolibrary.org/obo/go.owl http://purl.obolibrary.org/obo/cl.owl When the pipeline runs, it will download these ontology files from the specified remote sources and process them accordingly.","title":"Remote Ontology Files"},{"location":"advanced_config/#local-ontology-files","text":"For local ontology files that you want to include in the pipeline, you should follow these steps: Place the Files : Copy the local ontology files into the config/collectdata/local_ontologies directory. The files should have a .owl extension. No Need to List in vfb_fullontologies.txt : Unlike remote files, you do not need to add the local files to the vfb_fullontologies.txt file. The pipeline automatically detects any .owl files in the local_ontologies directory. Pipeline Processing : During the pipeline execution, any .owl files found in the local_ontologies directory will be copied to the VFB_DOWNLOAD_DIR . After this, the pipeline will proceed to download and process any remote ontology files listed in vfb_fullontologies.txt .","title":"Local Ontology Files"},{"location":"advanced_config/#example-configuration","text":"If your project requires both remote and local ontology files, your setup might look like this: Remote Files in vfb_fullontologies.txt : Local Files in config/collectdata/local_ontologies : Place files like my_local_ontology.owl and custom_terms.owl directly in the local_ontologies directory. This configuration ensures that your project uses a combination of remote and local ontology files, processed in a seamless and automated way during the pipeline run.","title":"Example Configuration"},{"location":"architecture/","text":"OBASK Architecture OBASK pipeline comprises three servers/services and four data pipelines: Pipeline servers : Triplestore Solr + preconfigured Solr core Search API ( ontology-search ) Neo4J production instance ( obask-kb ) Pipeline data pipelines : Data collection ( pipeline-collectdata ) Triplestore ingestion ( pipeline-updatetriplestore ) Data transformation and dumps for production instances ( pipeline-dumps ) KG production instance ingestion ( pipeline-updateprod ) Server and data pipelines are combined into 3 general sub-pipelines which are configured as Docker compose services. This documentation describes all 3 sub-pipelines in detail, including which role the individual servers and data pipelines play. Sub-pipeline: Deploy triplestore Summary : This pipeline deploys an empty triplestore, collects all relevant ontologies, and pre-processes and loads the collected data into the triplestore. Components: Triplestore pipeline-collectdata (data collection and preprocessing pipeline for all resources) pipeline-updatetriplestore (loading collected data into the triplestore) Dependents : pipeline-dumps Service: Triplestore Image : eclipse/rdf4j-workbench:3.7.7 Summary : The triplestore is currently an unspectacular default implementation of rdf4j-server. We make use of a simple in-memory store that is configured here . The container is maintained elsewhere (see docker-hub pages of image for details). Data pipeline: pipeline-collectdata Image : ghcr.io/obasktools/pipeline-collectdata pipeline-collectdata Git : https://github.com/OBASKTools/pipeline-collectdata Dockerfile Summary : This container encapsulates a process that downloads a number of source ontologies, and applies a number of ROBOT-based pre-processing steps, in particular: extracting modules/slices of external ontologies, running consistency checks and serialising as ttl for quicker ingest into triplestore. It also contains the data embargo pipeline and has some provisions for shacl validation . Detailed notes on pipeline-collectdata The process is encoded here . It performs the following steps: Downloading external ontologies. Removing embargoed data. The technique applied here is based on using ROBOT query and encoding the embargo logic as SPARQL queries (combined with ROBOT remove ). Ontologies in project configuration are imported in their entirety. Ontologies in slice configuration are sliced. The slice corresponds to a BOTTOM module that has the combined signature of all ontologies in the fullontologies section with the signature of the KB. All ontologies are converted to turtle format. All ontologies ready to be imported into the triplestore are gzipped. Data pipeline: pipeline-updatetriplestore Image : ghcr.io/obasktools/pipeline-updatetriplestore Dockerfile Git : https://github.com/OBASKTools/pipeline-updatetriplestore Summary : This container encapsulates a process that (1) sets up the triplestores and (2) loads all of the ttl files generated by pipeline-collectdata into the triplestore. The image contains the configuration details of triplestore, like choice of triplestore engine. Detailed notes on pipeline-updatetriplestore: The process loads the ontologies and data collected in the previous step into the triple store. Sub-pipeline: Data transformation and dumps for production instances (pipeline-dumps) Summary : This pipeline transforms the knowledge graph in the triplestore into various custom data dumps used by downstream services such as the production knowledge graph (neo4j) instance and solr. Depends on : Triplestore Dependents : obask-kb, solr, pipeline-updatesolr Data pipeline: pipeline-dumps Image : ghcr.io/obasktools/pipeline-dumps Git : https://github.com/OBASKTools/pipeline-dumps Summary : The dumps pipeline access the triple store to obtain data dumps that in mungs, transforms and enriches for various downstream purposes such as pipeline-prod ingestion and solr ingestion. Dockerfile Detailed notes on pipeline-dumps The process performs the following steps (all encoded in the Makefile ): Build dump for obask-kb (Neo4j tabular data structure) Build dump for solr (special json file, created using python) Sub-pipeline: Knowledge Graph (obask-kb) Summary : This pipeline deploys the production instance of the Knowledge Graph (neo4j database) and loads all the relevant data. Depends on : pipeline-dumps Dependents : None Service: obask-kb Image : ghcr.io/obasktools/obask-kb Git : https://github.com/OBASKTools/obask-kb Dockerfile Summary : Deploys an empty, configured instance of a Neo4J database with the neo2owl plugin , APOC and GDS tools. Data pipeline: pipeline-updateprod Image : ghcr.io/obasktools/pipeline-updateprod Git : https://github.com/OBASKTools/pipeline-updateprod Dockerfile Summary : The update-prod container currently takes an ontology (from the integration layer) and loads it into the the Neo4J instance (pipeline-prod) using the neo2owl plugin. Process: Loading the ontology using the neo4j2owl:owl2Import() procedure Setting a number of indices (see detailed notes below). Detailed notes about pipeline-updateprod You can set additional Pipeline post-processing steps like indices by editing this file . Note that this file can be used to set arbitrary post-processing cypher queries, not just indices (contrary to the file name). Essentially, all list cypher queries are executed in order right after PDB import is completed. The possible configuration settings for the neo4j2owl:owl2Import() procedure are described here . The configuration is stored here . Sub-pipeline: Ontology Search (ontology-search) Summary : This pipeline deploys the production instance of the Knowledge Graph (neo4j database) and loads all the relevant data. Depends on : pipeline-dumps Dependents : None Service: Solr Image : solr:8.11 Summary : Deploys an empty, configured instance of Solr. Data pipeline: pipeline-updatesolr Image : ghcr.io/obasktools/pipeline-updatesolr Git : https://github.com/OBASKTools/pipeline-updatesolr Dockerfile Summary : The pipeline-updatesolr container initializes the Solr schema and indexes the json data generated by the pipeline-dumps to the Solr. Service: ontology-search Image : ghcr.io/obasktools/ontology-search Git : https://github.com/OBASKTools/ontology-search Dockerfile Summary : Provides ontology search APIs.","title":"OBASK Architecture"},{"location":"architecture/#obask-architecture","text":"OBASK pipeline comprises three servers/services and four data pipelines: Pipeline servers : Triplestore Solr + preconfigured Solr core Search API ( ontology-search ) Neo4J production instance ( obask-kb ) Pipeline data pipelines : Data collection ( pipeline-collectdata ) Triplestore ingestion ( pipeline-updatetriplestore ) Data transformation and dumps for production instances ( pipeline-dumps ) KG production instance ingestion ( pipeline-updateprod ) Server and data pipelines are combined into 3 general sub-pipelines which are configured as Docker compose services. This documentation describes all 3 sub-pipelines in detail, including which role the individual servers and data pipelines play.","title":"OBASK Architecture"},{"location":"architecture/#sub-pipeline-deploy-triplestore","text":"Summary : This pipeline deploys an empty triplestore, collects all relevant ontologies, and pre-processes and loads the collected data into the triplestore. Components: Triplestore pipeline-collectdata (data collection and preprocessing pipeline for all resources) pipeline-updatetriplestore (loading collected data into the triplestore) Dependents : pipeline-dumps","title":"Sub-pipeline: Deploy triplestore"},{"location":"architecture/#service-triplestore","text":"Image : eclipse/rdf4j-workbench:3.7.7 Summary : The triplestore is currently an unspectacular default implementation of rdf4j-server. We make use of a simple in-memory store that is configured here . The container is maintained elsewhere (see docker-hub pages of image for details).","title":"Service: Triplestore"},{"location":"architecture/#data-pipeline-pipeline-collectdata","text":"Image : ghcr.io/obasktools/pipeline-collectdata pipeline-collectdata Git : https://github.com/OBASKTools/pipeline-collectdata Dockerfile Summary : This container encapsulates a process that downloads a number of source ontologies, and applies a number of ROBOT-based pre-processing steps, in particular: extracting modules/slices of external ontologies, running consistency checks and serialising as ttl for quicker ingest into triplestore. It also contains the data embargo pipeline and has some provisions for shacl validation .","title":"Data pipeline: pipeline-collectdata"},{"location":"architecture/#detailed-notes-on-pipeline-collectdata","text":"The process is encoded here . It performs the following steps: Downloading external ontologies. Removing embargoed data. The technique applied here is based on using ROBOT query and encoding the embargo logic as SPARQL queries (combined with ROBOT remove ). Ontologies in project configuration are imported in their entirety. Ontologies in slice configuration are sliced. The slice corresponds to a BOTTOM module that has the combined signature of all ontologies in the fullontologies section with the signature of the KB. All ontologies are converted to turtle format. All ontologies ready to be imported into the triplestore are gzipped.","title":"Detailed notes on pipeline-collectdata"},{"location":"architecture/#data-pipeline-pipeline-updatetriplestore","text":"Image : ghcr.io/obasktools/pipeline-updatetriplestore Dockerfile Git : https://github.com/OBASKTools/pipeline-updatetriplestore Summary : This container encapsulates a process that (1) sets up the triplestores and (2) loads all of the ttl files generated by pipeline-collectdata into the triplestore. The image contains the configuration details of triplestore, like choice of triplestore engine.","title":"Data pipeline: pipeline-updatetriplestore"},{"location":"architecture/#detailed-notes-on-pipeline-updatetriplestore","text":"The process loads the ontologies and data collected in the previous step into the triple store.","title":"Detailed notes on pipeline-updatetriplestore:"},{"location":"architecture/#sub-pipeline-data-transformation-and-dumps-for-production-instances-pipeline-dumps","text":"Summary : This pipeline transforms the knowledge graph in the triplestore into various custom data dumps used by downstream services such as the production knowledge graph (neo4j) instance and solr. Depends on : Triplestore Dependents : obask-kb, solr, pipeline-updatesolr","title":"Sub-pipeline: Data transformation and dumps for production instances (pipeline-dumps)"},{"location":"architecture/#data-pipeline-pipeline-dumps","text":"Image : ghcr.io/obasktools/pipeline-dumps Git : https://github.com/OBASKTools/pipeline-dumps Summary : The dumps pipeline access the triple store to obtain data dumps that in mungs, transforms and enriches for various downstream purposes such as pipeline-prod ingestion and solr ingestion. Dockerfile","title":"Data pipeline: pipeline-dumps"},{"location":"architecture/#detailed-notes-on-pipeline-dumps","text":"The process performs the following steps (all encoded in the Makefile ): Build dump for obask-kb (Neo4j tabular data structure) Build dump for solr (special json file, created using python)","title":"Detailed notes on pipeline-dumps"},{"location":"architecture/#sub-pipeline-knowledge-graph-obask-kb","text":"Summary : This pipeline deploys the production instance of the Knowledge Graph (neo4j database) and loads all the relevant data. Depends on : pipeline-dumps Dependents : None","title":"Sub-pipeline: Knowledge Graph (obask-kb)"},{"location":"architecture/#service-obask-kb","text":"Image : ghcr.io/obasktools/obask-kb Git : https://github.com/OBASKTools/obask-kb Dockerfile Summary : Deploys an empty, configured instance of a Neo4J database with the neo2owl plugin , APOC and GDS tools.","title":"Service: obask-kb"},{"location":"architecture/#data-pipeline-pipeline-updateprod","text":"Image : ghcr.io/obasktools/pipeline-updateprod Git : https://github.com/OBASKTools/pipeline-updateprod Dockerfile Summary : The update-prod container currently takes an ontology (from the integration layer) and loads it into the the Neo4J instance (pipeline-prod) using the neo2owl plugin. Process: Loading the ontology using the neo4j2owl:owl2Import() procedure Setting a number of indices (see detailed notes below).","title":"Data pipeline: pipeline-updateprod"},{"location":"architecture/#detailed-notes-about-pipeline-updateprod","text":"You can set additional Pipeline post-processing steps like indices by editing this file . Note that this file can be used to set arbitrary post-processing cypher queries, not just indices (contrary to the file name). Essentially, all list cypher queries are executed in order right after PDB import is completed. The possible configuration settings for the neo4j2owl:owl2Import() procedure are described here . The configuration is stored here .","title":"Detailed notes about pipeline-updateprod"},{"location":"architecture/#sub-pipeline-ontology-search-ontology-search","text":"Summary : This pipeline deploys the production instance of the Knowledge Graph (neo4j database) and loads all the relevant data. Depends on : pipeline-dumps Dependents : None","title":"Sub-pipeline: Ontology Search (ontology-search)"},{"location":"architecture/#service-solr","text":"Image : solr:8.11 Summary : Deploys an empty, configured instance of Solr.","title":"Service: Solr"},{"location":"architecture/#data-pipeline-pipeline-updatesolr","text":"Image : ghcr.io/obasktools/pipeline-updatesolr Git : https://github.com/OBASKTools/pipeline-updatesolr Dockerfile Summary : The pipeline-updatesolr container initializes the Solr schema and indexes the json data generated by the pipeline-dumps to the Solr.","title":"Data pipeline: pipeline-updatesolr"},{"location":"architecture/#service-ontology-search","text":"Image : ghcr.io/obasktools/ontology-search Git : https://github.com/OBASKTools/ontology-search Dockerfile Summary : Provides ontology search APIs.","title":"Service: ontology-search"},{"location":"features/","text":"OBASK Features","title":"Features"},{"location":"features/#obask-features","text":"","title":"OBASK Features"},{"location":"quick_start/","text":"OBASK Quick start Prerequisites: Update to Docker Compose V2 (>= Docker Compose version v2.17) (see https://docs.docker.com/compose/install/linux/ ) Create your project: Install cookiecutter bash python3 -m pip install --user cookiecutter or bash conda install cookiecutter Navigate to a folder where you want to create your new pipeline project and run the OBASK project template: bash cookiecutter gh:OBASKTools/obask-template or bash python3 -m cookiecutter gh:OBASKTools/obask-template Then provide your project_name when asked. Commit your project to GitHub. Be bold; if you are not satisfied with the result, you can delete the repository and create it again as many times as you want. Navigate to your newly created project folder and initialize the git repository. bash git init -b main git add . git commit -m \"First commit\" Create a new repository . The repo name should be the same as the project_name you provided to the template. Do not initialize with a README (you already have one). Click Create. See the section under \"\u2026or push an existing repository from the command line.\" Follow the instructions there. For example, (make sure the location of your remote is exactly correct!). bash git remote add origin https://github.com/MyRepo/my_project_name.git git branch -M main git push -u origin main Customize your configs: Mandatory: - config/collectdata/vfb_fullontologies.txt - config/prod/neo4j2owl_config.yaml (see neo4j2owl configuration ) Advanced: - collect/sparqls - dumps/sparqls for labels Run your project: When ready, run: docker-compose up Once the pipeline is complete, your services will be live at: - Neo4J, http://localhost:7474/browser/ - Solr, http://localhost:8993/solr - Ontology API, http://localhost:8007/ontology - Triplestore, http://localhost:8080/rdf4j-workbench/repositories/obask/summary Re-run your project If you are not 100% happy with the output and want to do further configurations, you can safely shutdown and clean all services through: docker-compose down Then you can continue customizing your configurations and run the pipeline again to see them in action via: docker-compose up Troubleshooting 1- Dumps: Semantic tag labels should not contain a space character in the config/prod/neo4j2owl_config.yaml 1- Dumps: Parser Exception with a message similar to: Caused by: org.semanticweb.owlapi.manchestersyntax.renderer.ParserException: Encountered RO_0002100 at line 1 column 1. Expected one of: Class name Object property name Data property name inverse not This error indicates that the mentioned entity (in this case RO_0002100 ) is used in the config/prod/neo4j2owl_config.yaml , but it could not be found in any of the input ontologies ( config/collectdata/vfb_fullontologies.txt ). Problem in this case is a typographical error: RO_0002100 should be used in its curie form RO:0002100 in the config/prod/neo4j2owl_config.yaml .","title":"Project quick start"},{"location":"quick_start/#obask-quick-start","text":"","title":"OBASK Quick start"},{"location":"quick_start/#prerequisites","text":"Update to Docker Compose V2 (>= Docker Compose version v2.17) (see https://docs.docker.com/compose/install/linux/ )","title":"Prerequisites:"},{"location":"quick_start/#create-your-project","text":"Install cookiecutter bash python3 -m pip install --user cookiecutter or bash conda install cookiecutter Navigate to a folder where you want to create your new pipeline project and run the OBASK project template: bash cookiecutter gh:OBASKTools/obask-template or bash python3 -m cookiecutter gh:OBASKTools/obask-template Then provide your project_name when asked. Commit your project to GitHub. Be bold; if you are not satisfied with the result, you can delete the repository and create it again as many times as you want. Navigate to your newly created project folder and initialize the git repository. bash git init -b main git add . git commit -m \"First commit\" Create a new repository . The repo name should be the same as the project_name you provided to the template. Do not initialize with a README (you already have one). Click Create. See the section under \"\u2026or push an existing repository from the command line.\" Follow the instructions there. For example, (make sure the location of your remote is exactly correct!). bash git remote add origin https://github.com/MyRepo/my_project_name.git git branch -M main git push -u origin main Customize your configs: Mandatory: - config/collectdata/vfb_fullontologies.txt - config/prod/neo4j2owl_config.yaml (see neo4j2owl configuration ) Advanced: - collect/sparqls - dumps/sparqls for labels","title":"Create your project:"},{"location":"quick_start/#run-your-project","text":"When ready, run: docker-compose up Once the pipeline is complete, your services will be live at: - Neo4J, http://localhost:7474/browser/ - Solr, http://localhost:8993/solr - Ontology API, http://localhost:8007/ontology - Triplestore, http://localhost:8080/rdf4j-workbench/repositories/obask/summary","title":"Run your project:"},{"location":"quick_start/#re-run-your-project","text":"If you are not 100% happy with the output and want to do further configurations, you can safely shutdown and clean all services through: docker-compose down Then you can continue customizing your configurations and run the pipeline again to see them in action via: docker-compose up","title":"Re-run your project"},{"location":"quick_start/#troubleshooting","text":"1- Dumps: Semantic tag labels should not contain a space character in the config/prod/neo4j2owl_config.yaml 1- Dumps: Parser Exception with a message similar to: Caused by: org.semanticweb.owlapi.manchestersyntax.renderer.ParserException: Encountered RO_0002100 at line 1 column 1. Expected one of: Class name Object property name Data property name inverse not This error indicates that the mentioned entity (in this case RO_0002100 ) is used in the config/prod/neo4j2owl_config.yaml , but it could not be found in any of the input ontologies ( config/collectdata/vfb_fullontologies.txt ). Problem in this case is a typographical error: RO_0002100 should be used in its curie form RO:0002100 in the config/prod/neo4j2owl_config.yaml .","title":"Troubleshooting"}]}